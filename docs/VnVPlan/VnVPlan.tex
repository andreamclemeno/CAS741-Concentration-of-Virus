\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}


% from SRS
\usepackage{amsmath, mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{xr}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{xfrac}
\usepackage{tabularx}
\usepackage{float}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{pdflscape}
\usepackage{afterpage}
%%%
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\newcounter{tinnum} %Likely change number
\newcommand{\lthetinnum}{T\thetinnum}
\newcommand{\tinref}[1]{T-\ref{#1}}



\begin{document}

\title{System Verification and Validation 
Plan for \progname{}} 
\author{Andrea Clemeno}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
10/29/2020  & 1.0 & First Draft of VnV\\
10/31/2020  & 1.1 & Updating first draft\\
12/05/2020  & 1.2 & Updating with test cases\\
12/10/2020  & 1.3 & Updating to address comments\\
12/12/2020  & 1.4 & Updating to Dr. Smith's comments\\
\bottomrule
\end{tabularx}


\newpage

\tableofcontents
\newpage
\listoftables
\newpage

\newpage

\section{Symbols, Abbreviations and Acronyms}

The following tables identify the symbols, abbreviations and acronyms use 
throughout this document.
  
\subsection{Table of Symbols, Abbreviations and Acronyms}

The table that follows summarizes the symbols used in this document along with
their descriptions. The symbols are listed in alphabetical order. In addition, 
all symbols, abbreviations, and acronyms recorded in the SRS for \progname{} 
\citep{SRS} apply to this document.


\begin{table}[H]
\renewcommand{\arraystretch}{1.2}
%\noindent \begin{tabularx}{1.0\textwidth}{l l X}
\noindent \begin{longtable*}{l p{8cm}} \toprule
\textbf{symbol} & \textbf{description}\\
\midrule 
API & Application Program Interface 
\\
N/A & Not Applicable
\\
OS & Operating System
\\
SRS & Software Requirements Specification
\\
T & Test 
\\
TC & Test Case 
\\
VnV & Verification and Validation 
\\
&\\
\bottomrule

\end{longtable*}
\caption{Table of Symbols, Abbreviations and Acronyms}
\end{table}


\newpage

\pagenumbering{arabic}

This document outlines the verification and validation plans for components 
significant for the implementation of the \progname{} program, including the 
SRS, design, implementation and software validation. Following this, the system 
tests for functional and non-functional requirements are indicated. Moreover, 
how test cases meet requirements will be identified. After describing the system 
testing, the unit testing for all requirements using different modules will be 
explained as well as the traceability from the test cases to the modules.

\section{General Information}

\subsection{Summary}

The software under tested works with viral load concentrations from patients 
infected with the HIV-1 virus to determine the efficiency of their immune 
system. 
The efficiency can be described by the elimination rate of the virus in the 
patient's body.
This rate of change will help predict the viral load concentration of the 
patients after a user-defined number of days. 

\subsection{Objectives}

The objective of \progname{} is to provide predictions that are tested and 
verified. The various intended users of the software will need to view the 
results easily. Lastly, the software should be optimized and provided timely. 
The following VnV plan will build confidence in the software correctness, 
demonstrate adequate portability for the intended users, and ensure efficient 
software performance.


\subsection{Relevant Documentation}

The \progname{} program will use different documentation to identify its 
purpose and the development methods used. The documentation includes the SRS 
(\citet{SRS}), the following VnV Plan, Drasil documentation (\citet{DrasilSRS})
and VnV Report (\citet{DiagnoseVNVreport}).



\section{Plan}
	
\subsection{Verification and Validation Team}

Verification and validation are used to build confidence in the correctness of 
\progname{}. The following document will be reviewed by: the project developer, 
Andrea Clemeno, the domain expert reviewer, Elizabeth Hofer, and the secondary 
reviewer, John Ernsthausen. In addition, Dr.\ Smith, the CAS 741 course 
instructor, will review the VnV plan. The reviewers will ensure the document is 
in accordance with the VnV plan checklist (\citet{Vnvchecklist}). The feedback 
is provided using issues on GitHub repository: CAS741-Diagnose. 

\subsection{SRS Verification Plan}

The verification of the SRS will be done to ensure that the requirements 
specified are in alignment with the outlined objective of the \progname{} 
program. The SRS verification plan will involve reviewing the document against 
the SRS checklist and providing feedback using issues on GitHub repository 
(\citet{SRSchecklist}). The results will be summarized in the VnV report 
(\citep(DiagnoseVNVreport)). The 
reviewers that will verify the SRS document include: the class instructor, Dr. 
Smith;  the domain expert reviewer, Elizabeth Hofer; the secondary reviewer, 
Tiago de Moraes Machado; the project developer, Andrea Clemeno.

\subsection{Design Verification Plan}

The specifications and implementation of the software will be documented 
extensively with several reports including the SRS, VnV plan and VnV report. 
However, the design will not be documented as the modules of the code will be 
generated by the Drasil Framework. The generation of the design will be modular, 
correctness is assumed and verification is not required.

\subsection{Implementation Verification Plan}

The implementation of the software will be verified using several techniques 
involving manual or automated interactions. The software will be generated using 
the Drasil Framework to generate some of the software artifacts, specifically 
the SRS and python code \citep{Drasil}. The generated code implements python to 
achieve goals and fulfill the requirements outlined in the SRS. The 
design of the code will be evaluated by the project developer, domain expert, 
the secondary reviewer and the class instructor. 

The code evaluation will involve unit and system testing, static and dynamic 
code analysis, linting and continuous integration. The tools for the evaluations 
will be mentioned in section \ref{tools}. Furthermore, the test cases used in 
system and unit testing will be stated in sections \ref{systestcases} and 
\ref{unittestcases} respectively. 


\subsection{Automated Testing and Verification Tools}\label{tools}

The \progname{} software will be tested and verified with several tools for 
unit and systems testing, static and dynamic analysis, linting and continuous 
integration. The static automatic testing will be completed in Spyder, a Python 
Integrated Development Environment, using several checkpoints with success and 
failure indicators. Additionally, the Spyder platform will analyze the code for 
potential errors in the process of linting. For testing performance optimization 
dynamically, a python profiler called cProfile will be used to profile speed 
(\citet{cProfile}).

Automated testing will be implemented for individual units as well as the 
integrated system to ensure that all the sections work separately and together 
seamlessly. In terms of unit  testing, each unit will be tested with the 
unittest python library within Spyder. Respectively, the systems test will be 
completed through black box testing with Pytest.

Lastly, the implementation of the design will be verified with continuous 
integration through Travis CI that is used in conjunction with Drasil. Drasil 
implements Travis CI to integrate code into a Github repository each day to 
complete automated tests to verify the code (\citet{Drasil}). 

\begin{center}
 \begin{tabular}{||c|c||} 
 \hline
 \textbf{Test} & \textbf{Verification Tool}\\ [0.5ex] 
 \hline
  Static Analysis & Spyder\\
 \hline
  Linting & Spyder\\
 \hline
  Dynamic Analysis & cProfile \\
 \hline
  System Test & Pytest\\
 \hline
  Unit Test & unittest \\
 \hline
  Continuous Integration & Travis CI\\ [1ex] 
 \hline
\end{tabular}
\captionof{table}{Automated Testing and Verification Tools}
\end{center}		

\newpage

\subsection{Software Validation Plan}

The software validation plan will be implemented at the end of the development 
process to determine if the real world problem is characterized correctly. The 
validation of \progname{} will be completed by comparing the outputs of the 
software to several cases from scientific study called Modeling plasma virus 
concentration during primary HIV infection seen in The Journal of Theoretical 
Biology
(\citet{Stafford2000}).

\section{System Test Description} \label{systestcases}

\subsection{Tests for Functional Requirements} \label{tfr}

This section will define the tests to ensure \progname{} meets the functional 
requirements seen in the SRS document for \progname{} (\citep{SRS}). The 
subsections combine 
several requirements that are be separated based on common ideas. 


\subsubsection{Input Verification}

The inputs will undergo tests to satisfy R1 and R2 from \progname{} SRS. 
Specifically ensuring values are input and align with the input constraints. 

Table \ref{tabletestcases} displays the inputs and outputs of test cases for the 
input constraints test T-X-Y.

\begin{center}
 \begin{tabular}{|| c||c c c c|| c ||} 
 \hline
 \textbf{Test} & \textbf{$N_{o}$ ($\frac{mol}{mL}$)} & \textbf{$N_{t}$ ($\frac{mol}{mL}$)} & \textbf{$t_{t}$ (d)} & \textbf{$t_{p}$ (d)} & \textbf{Output}\\ [0.5ex] 
 \hline
 TC-1-1 & 12000000 & 5000000 & 1 & 30 & - \\ 
 \hline
 TC-2-1 & 0 & 5000000 & 1 & 30 & Exception: InputError\\
 \hline
 TC-2-2 & -12000000 & 5000000 & 1 & 30 & Exception: InputError\\
 \hline
 TC-3-1 & 12000000 & 0 & 1 & 30 & Exception: InputError\\
 \hline
 TC-3-2 & 12000000 & -10000000 & 1 & 30 & Exception: InputError\\ 
 \hline
 TC-4-1 & 5000000 & 12000000 & 1 & 30 & Exception: InputError\\ 
 \hline
 TC-5-1 & 12000000 & 5000000 & 0 & 30 & Exception: InputError\\
 \hline
 TC-5-2 & 12000000 & 5000000 & -1 & 30 & Exception: InputError\\
 \hline
 TC-6-1 & 12000000 & 5000000 & 1 & 0 & Exception: InputError\\
 \hline
 TC-6-2 & 12000000 & 5000000 & 1 & -1 & Exception: InputError\\ 
 \hline
 TC-7-1 & 12000000 & 5000000 & 30 & 1 & Exception: InputError\\  [1ex] 
 \hline
\end{tabular}
\captionof{table}{Test Cases used for Verification of 
\progname{}}\label{tabletestcases}

\end{center}		
\paragraph{Input Constraints Test}

\begin{enumerate}

\item{T-}\refstepcounter{tinnum}\thetinnum\label{T-1}:

Control: Automatic
					
Initial State: N/A
					
Input: $N_{o}$ = 12000000 $\frac{mol}{mL}$; $N_{t}$= 5000000 $\frac{mol}{mL}$; 
$t_{t}$ = 1 d; $t_{p}$ = 30 d (TC-1-1)
					
Output: $k$ = 0.8755; $N_p$ = 4.708e-05

Test Case Derivation: Inputs within expected input range are used in the 
previous test to ensure successful input.
					
How test will be performed: Pytest will be implemented.
					
\item{T-}\refstepcounter{tinnum}\thetinnum\label{T-2}:

Control: Automatic
					
Initial State: N/A
					
Input: TC-2-1,TC-2-2
					
Output: Exception: InputError

Test Case Derivation: $N_{o}$ input is out of bounds and not an expected input 
value. TC-2-1 and TC-2-2 have zero and negative inputs for $N_{o}$  
respectively. 

How test will be performed: Pytest will be implemented.


\item{T-}\refstepcounter{tinnum}\thetinnum\label{T-3}:

Control: Automatic
					
Initial State: N/A
					
Input: TC-3-1,TC-3-2
					
Output: Exception: InputError

Test Case Derivation: $N_{t}$ input is out of bounds and not an expected input 
value. TC-3-1 and TC-3-2 have zero and negative inputs for $N_{t}$ respectively. 

How test will be performed: Pytest will be implemented.


\item{T-}\refstepcounter{tinnum}\thetinnum\label{T-4}:

Control: Automatic
					
Initial State: N/A
					
Input: TC-4-1
					
Output: Exception: InputError

Test Case Derivation: Relationship between $N_o$ and $N_{t}$ is invalid and out 
of bounds and not an expected input values.

How test will be performed: Pytest will be implemented.


\item{T-}\refstepcounter{tinnum}\thetinnum\label{T-5}:

Control: Automatic
					
Initial State: N/A
					
Input: TC-5-1, TC-5-2
					
Output: Exception: InputError

Test Case Derivation: $t_{t}$ input is out of bounds and not an expected input 
value. TC-5-1 and TC-5-2 have zero and negative inputs for $t_{t}$ respectively. 

How test will be performed: Pytest will be implemented.


\item{T-}\refstepcounter{tinnum}\thetinnum\label{T-6}:

Control: Automatic
					
Initial State: N/A
					
Input: TC-6-1, TC-6-2
					
Output: Exception: InputError

Test Case Derivation: $t_{p}$ input is out of bounds and not an expected input 
value. TC-6-1 and TC-6-2 have zero and negative inputs for $t_{p}$ respectively. 

How test will be performed: Pytest will be implemented.


\item{T-}\refstepcounter{tinnum}\thetinnum\label{T-7}:

Control: Automatic
					
Initial State: N/A
					
Input: TC-7-1
					
Output: Exception: InputError

Test Case Derivation: Relationship between $t_t$ and $t_{p}$ is invalid and out 
of bounds and not an expected input values.

How test will be performed: Pytest will be implemented.
 

\end{enumerate}

\subsubsection{Output Verification}\label{TO-1-1}

\begin{enumerate}

\item{T-}\refstepcounter{tinnum}\thetinnum\label{T-8}:

Control: Automatic
					
Initial State: N/A
					
Input: $N_{o}$ = 12000000 $\frac{mol}{mL}$; $N_{t}$= 5000000 $\frac{mol}{mL}$; 
$t_{t}$ = 1 d; $t_{p}$ = 30 d (TC-1-1)
					
Output: $k$ = 0.8755; $N_p$ = 4.708e-05

Test Case Derivation: Inputs within expected input range are used in the 
previous test to ensure the correct values produce an output.
					
How test will be performed: Pytest will be implemented.


\end{enumerate}


\subsection{Tests for Nonfunctional Requirements} \label{tnfr}
  
This section will define the tests to ensure \progname{} fulfill the 
nonfunctional requirements seen in the SRS document of \progname{}. The 
subsections are be separated for different requirements, including 
verifiability, understandable, reusable, maintainable, and portable.

\subsubsection{Verifiability}
		
\paragraph{Testing the verifiability of \progname{}:}
\begin{enumerate}

\item{T-}\refstepcounter{tinnum}\thetinnum\label{T-9}:

Type: Manual 
					
Initial State: Functional requirements system testing completed.
					
Condition: Software must output expected values and errors stated in section 
\ref{tfr}.
					
Result: Functional requirements of SRS are verified using unit and system testing.
					
How test will be performed: The test will be implemented manually inspecting the 
completion on the tests outlined in sections \ref{tfr} and \ref{utnfr}. A successful test is specified by the verification of each test for functional requirements.
					
\item{T-}\refstepcounter{tinnum}\thetinnum\label{T-10}:

Type: Manual 
					
Initial State: Non-functional requirements testing completed.
					
Condition: Software must pass tests for non-functional requirements in section 
\ref{tfr}. 
					
Result: Non-functional requirements of SRS are verified using system testing.
					
How test will be performed: The test will be implemented manually inspecting the 
completion on the tests outlined in sections \ref{nonfunc_understability}, 
\ref{nonfunc_reliability},  \ref{nonfunc_maintainability} and 
\ref{nonfunc_portability}. A successful test is specified by the verification of each test for non-functional requirements in section \ref{tnfr}.
					

\end{enumerate}

\subsubsection{Understandability} \label{nonfunc_understability}

\paragraph{Testing the understandability of \progname{}:}
\begin{enumerate}

\item{T-}\refstepcounter{tinnum}\thetinnum\label{T-11}:

Type: Automatic
					
Initial State: Python code generated.
					
Input: Generated modular python code and TC-1-1 from table \ref{tabletestcases}.
					
Output: Convention, refactor, warning and error messages.
					
How test will be performed: Test will be performed by linting using Spyder. A successful test is specified by verification of no error messages.
				

\end{enumerate}

\subsubsection{Reliability} \label{nonfunc_reliability}
\paragraph{Testing the reliability of \progname{}:}

\begin{enumerate}

\item{T-}\refstepcounter{tinnum}\thetinnum\label{T-12}:

Type: Automatic
					
Initial State: \progname{} software run completed.
					
Input: The code generated by Drasil and cProfile command.

Result: The total run time, wall time per function call, the cumulative time for a given function.
			
					
How test will be performed: \\
The test for performance will use the cProfile library 
atuomatically in Spyder to identify lines of the code that can possibly be 
optimized. The system will be profiled using ProfileC to determine the wall time 
per function call and cumulative time spent on a given function. A successful test is specified by a total run time of less than 0.5 seconds. \\
\end{enumerate}

\subsubsection{Maintainability} \label{nonfunc_maintainability}
\paragraph{Testing the maintainability of \progname{}:}

\begin{enumerate}

\item{T-}\refstepcounter{tinnum}\thetinnum\label{T-13}:

Type: Manual
					
Initial State: Completed documentation for \progname{}, including SRS, VnV plan 
and VnV report.
					
Input/Condition: Traceability Matrices in completed documentation.
					
Output/Result: Traceability to Requirements confirmed. 
					
How test will be performed: This test will be inspected to ensure that the tests 
can be traced to the requirements and the requirements can be traced to the 
goals of \progname{}. A successful test is specified by the verification of connection between each test case to a requirement ensuring all requirements are met.
					

\end{enumerate}

\subsubsection{Portability} \label{nonfunc_portability}

\paragraph{Testing the portability of \progname{}:}

\begin{enumerate}

\item{T-}\refstepcounter{tinnum}\thetinnum\label{T-14}:

Type: Manual
					
Initial State: andreamclemeno/Drasil Github repository cloned in OS.
					
Condition: Perform T-1 test from section \ref{TO-1-1} by running the make command in the Drasil/code directory in terminal.
					
Result: Successful test implies portability of software.

How test will be performed: The test will be performed manual by executing the 
make command in terminal of different OS, including Linux, Windows, MacOS.  A successful test is specified by the verification of each test for functional requirements. A successful test is specified by the GENERATED OUTPUT MATCHES STABLE VERSION indicator in terminal.
					
\end{enumerate}


\subsection{Traceability Between Test Cases and Requirements}

The purpose of the traceability matrices is to provide easy references on what
has to be additionally modified if a certain component is changed.  Every time a
component is changed, the items in the column of that component that are marked
with an ``X'' may have to be modified as well.  Table~\ref{Table:R_trace} shows 
which test cases are supporting which requirements.



\begin{table}[ht]
\centering
\begin{tabular}{||c||c|c|c|c|c|c|c|c|c|c||}
\hline
	& R1 & R2 & R3 & R4 & R5 & NF1 & NFR2 & NFR3 & NFR4 & NFR5 \\
\hline
\tinref{T-1}        	& X& X& X& X& X& & & & & \\
\hline
\tinref{T-2}			& X& X& & & & & & & &\\
\hline
\tinref{T-3}        	& X& X& & & & & & & &\\
\hline
\tinref{T-4}			& X& X& & & & & & &  &\\
\hline
\tinref{T-5}        	& X& X& & & & & & & &\\
\hline
\tinref{T-6}			& X& X& & & & & & & &\\
\hline
\tinref{T-7}        	& X& X& & & & & & & &\\
\hline
\tinref{T-8}			& X& X&X & X& X& & & & &\\ 
\hline
\tinref{T-9}        	& & & & & &X & & & &\\
\hline
\tinref{T-10}			& & & & & &X & & & &\\
\hline
\tinref{T-11}        	& & & & & & &X & & &\\
\hline
\tinref{T-12}			& & & & & & & &X & &\\ 
\hline
\tinref{T-13}        	& & & & & & & & &X &\\
\hline
\tinref{T-14}			& & & & & & & & & &X\\
\hline
\tinref{T-15}        	& & &X & & & & & & &\\
\hline
\tinref{T-16}			&X &X &X &X &X & & & & &\\
\hline

\end{tabular}
\caption{Traceability Matrix Showing the Connections Between Requirements and 
Tests}
\label{Table:R_trace}
\end{table}


\section{Unit Test Description} \label{unittestcases}


The design of the generated python code into modules can be attributed to the 
Drasil framework. The documentation of the knowledge captured by Drasil to 
generate this code can be found in the Github repository: andreamclemeno/Drasil. 
The plans for unit testing the modules will be outlined in the following 
section.


\subsection{Unit Testing Scope}

The modules generated by Drasil for \progname{} include: Calculations.py, 
Control.py, InputConstraints.py, InputFormat.py, InputParameter.py and 
OutputFormat.py. Although the modules are generated, system or unit testing will 
be completed for all the modules to ensure verifiability. The testing type, 
verification tool and test reference can be found in Table \ref{tablemod}. Test 
reference names the test(s) where the testing plan can be found.

\begin{center}
 \begin{tabular}{||c|c|c|c||} 
 \hline
 Module & Type & Verification Tool & Test Reference \\ [0.5ex] 
 \hline
  Calculations.py & unit & unittest in Python & \tinref{T-15}\\
  \hline
  Control.py & unit & unittest in Python & \tinref{T-16}\\
  \hline
  InputConstraints.py & system & Pytest & \tinref{T-1}, \tinref{T-8}\\
  \hline
  InputFormat.py & system & Pytest & \tinref{T-1}, \tinref{T-8}\\
  \hline
  InputParameter.py & system & Pytest & \tinref{T-1}, \tinref{T-8}\\
  \hline
  OutputFormat.py & system & Pytest & \tinref{T-1}, \tinref{T-8}\\
 \hline
\end{tabular}
\captionof{table}{\progname{} Module Testing Type and Verification Tools}
\label{tablemod}

\end{center}	


\subsection{Tests for Functional Requirements}\label{utnfr}

The verification for the functional requirements will be completed through 
automated testing using methods outlined in Table \ref{tablemod}. Unit testing 
processes are outlined for Calculations.py in section \ref{modcalc} and for 
Control.py in section \ref{modcontrol}.

\subsubsection{Calculations.py}\label{modcalc}

\begin{enumerate}

\item{T-}\refstepcounter{tinnum}\thetinnum\label{T-15}:

Type: Automatic
					
Initial State: Calculations.py generated from Drasil and test\_Calculations.py 
created.
					
Input: test\_Calculations.py 
					
Output: Terminal output with OK indicator

Test Case Derivation: Exceptions raised by Calculations.py are possible. If unittest fails because an exception is raised, Calculations.py is not behaving correctly. This test case was derived to determine behaviour irregularities.

How test will be performed: Test is performed by implementing unittest in the 
terminal. In the directory of the test\_Calculations.py code, the following 
command will be executed: 
\begin{center}
python3 -m unittest test\_Calculations.py
\end{center}

The confirmation of OK indicator will specify a successful test.
					
    
\end{enumerate}

\subsubsection{Control.py}\label{modcontrol}
\begin{enumerate}
\item{T-}\refstepcounter{tinnum}\thetinnum\label{T-16}:

Type: Automatic
					
Initial State: Control.py generated from Drasil and test\_Control.py 
created.
					
Input: test\_Control.py
					
Output: Terminal output with OK indicator

Test Case Derivation: Exceptions raised by Control.py are possible. If unittest fails because an exception is raised, Control.py is not behaving correctly. This test case was derived to determine behaviour irregularities.

How test will be performed: Test is performed by implementing unittest in the 
terminal. In the directory of the test\_Control.py code, the following 
command will be executed: 
\begin{center}
python3 -m unittest test\_Control.py
\end{center}

The confirmation of OK indicator will specify a successful test.
			
					
    
\end{enumerate}


\subsection{Tests for Nonfunctional Requirements}

Planning for nonfunctional tests of units will not be that relevant for 
\progname{}.

\subsection{Traceability Between Test Cases and Modules}

The purpose of the traceability matrices is to provide easy references on what
has to be additionally modified if a certain component is changed.  Every time a
component is changed, the items in the column of that component that are marked
with an ``X'' may have to be modified as well.  Table~\ref{Table:M_trace} shows 
which test are supporting which modules.

\begin{landscape}
\begin{table}[ht]
\centering
\begin{tabular}{||c||c|c|c|c|c|c|c|c|c|c||}
\hline
	& Calculations.py & Control.py & InputConstraints.py & InputFormat.py & 
InputParameter.py & OutputFormat.py \\
\hline
\tinref{T-1}        	&X &X &X &X &X &X  \\
\hline
\tinref{T-2}			& & & & & &  \\
\hline
\tinref{T-3}        	& & & & & &  \\
\hline
\tinref{T-4}			& & & & & &  \\
\hline
\tinref{T-5}        	& & & & & &  \\
\hline
\tinref{T-6}			& & & & & &  \\
\hline
\tinref{T-7}			& & & & & &  \\
\hline
\tinref{T-8}			&X &X &X &X &X &X  \\
\hline
\tinref{T-9}			& & & & & &  \\
\hline
\tinref{T-10}			& & & & & &  \\
\hline
\tinref{T-11}			& & & & & &  \\
\hline
\tinref{T-12}			& & & & & &  \\
\hline
\tinref{T-13}			& & & & & &  \\
\hline
\tinref{T-14}			& & & & & &  \\
\hline
\tinref{T-15}			& & &X & & &  \\
\hline
\tinref{T-16}			&X &X &X &X &X &X \\
\hline

\end{tabular}
\caption{Traceability Matrix Showing the Connections Between Modules and 
Tests}
\label{Table:M_trace}
\end{table}
\end{landscape}
				
\bibliographystyle{plainnat}

\bibliography{references.bib}
~\newpage

\end{document}











