\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}


% from SRS
\usepackage{amsmath, mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{xr}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{xfrac}
\usepackage{tabularx}
\usepackage{float}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{pdflscape}
\usepackage{afterpage}
%%%
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\newcounter{tinnum} %Likely change number
\newcommand{\lthetinnum}{Tinput\thetinnum}
\newcommand{\tinref}[1]{Tinput\ref{#1}}


\begin{document}

\title{Medical Diagnosis Prediction Tool: System Verification and Validation 
Plan for \progname{}} 
\author{Andrea Clemeno}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
10/29/2020  & 1.0 & First Draft of VnV\\
10/31/2020  & 1.1 & Updating first draft\\
12/05/2020  & 1.2 & Updating with test cases\\
12/10/2020  & 1.3 & Updating to address comments\\
\bottomrule
\end{tabularx}


\newpage

\tableofcontents
~\newpage
\listoftables
~\newpage

\newpage

\section{Symbols, Abbreviations and Acronyms}

The following tables identify the symbols, abbreviations and acronyms use 
throughout this document.
  
\subsection{Table of Symbols}

The table that follows summarizes the symbols used in this document along with
their descriptions. The symbols are listed in alphabetical order. In addition, 
all symbols, abbreviations, and acronyms recorded in the SRS for \progname{} 
\citep{SRS} apply to this document.


\begin{table}[H]
\renewcommand{\arraystretch}{1.2}
%\noindent \begin{tabularx}{1.0\textwidth}{l l X}
\noindent \begin{longtable*}{l p{8cm}} \toprule
\textbf{symbol} & \textbf{description}\\
\midrule 
API & Application Program Interface 
\\
SRS & Software Requirements Specification
\\
T & Test 
\\
VnV & Verification and Validation 
\\
&\\
\bottomrule

\end{longtable*}
\caption{Table of Symbols}
\end{table}

\newpage


\subsection{Abbreviations and Acronyms}

The table that follows summarizes the symbols used in this document that allude 
to different sections of the Software Requirements Specification. The symbols 
are listed in alphabetical order.
\begin{table}[ht]
\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  A & Assumption\\
  DD & Data Definition\\
  GD & General Definition\\
  GS & Goal Statement\\
  IM & Instance Model\\
  LC & Likely Change\\
  PS & Physical System Description\\
  R & Requirement\\
  SRS & Software Requirements Specification\\
  \progname{} & Medical Diagnosis Prediction Tool \\
   & for Acquired immunodeficiency syndrome (AIDS)\\
  T & Theoretical Model\\
  ULC & Unlikely Change\\
  \bottomrule
\end{tabular}\\
\end{center}
\caption{Table of Abbreviations and Acronyms}

\end{table}

\newpage

\pagenumbering{arabic}

This document outlines the verification and validation plans for components 
significant for the implementation of the \progname{} program, including the 
SRS, design, implementation and software validation. Following this, the system 
tests for functional and non-functional requirements are indicated. Moreover, 
how test cases meet requirements will be identified. After describing the system 
testing, the unit testing for all requirements using different modules will be 
explained as well as the traceability from the test cases to the modules.

\section{General Information}

\subsection{Summary}

The software under tested works with viral load concentrations from patients infected with the HIV-1 virus to determine the efficiency of their immune system. 
The efficiency can be described by the elimination rate of the virus in the patient's body.
This rate of change will help predict the viral load concentration of the patients after a user-defined number of days. 

\subsection{Objectives}

The objective of \progname{} is to provide predictions that are tested and verified. The various intended users of the software will need to view the results easily. Lastly, the software should be optimized and provided timely. The following VnV plan will build confidence in the software correctness, demonstrate adequate portability for the intended users, and ensure efficient software performance.


\subsection{Relevant Documentation}

The \progname{} program will use different documentation to identify its 
purpose and the development methods used. The documentation includes the SRS 
(\citet{SRS}), the following VnV Plan, Drasil documentation (\citet{DrasilSRS})
and VnV Report (\citet{DiagnoseVNVreport}).



\section{Plan}
	
\subsection{Verification and Validation Team}

Verification and validation are used to build confidence in the correctness of \progname{}. The following document will be reviewed by: the primary reviewer, 
Andrea Clemeno, the domain expert reviewer, Elizabeth Hofer, and the secondary 
reviewer, John Ernsthausen. In addition, Dr.\ Smith, the CAS 741 course 
instructor, will review the VnV plan. The reviewers will ensure the document is 
in accordance with the VnV plan checklist (\citet{Vnvchecklist}). 

\subsection{SRS Verification Plan}

The verification of the SRS will be done to ensure that the requirements 
specified are in alignment with the outlined objective of the \progname{} 
program. The SRS verification plan  will involve reviewing the document against 
the SRS checklist and providing feedback using issues on GitHub repository 
(\citet{SRSchecklist}). The 
reviewers that will verify the SRS document include: the class instructor, Dr. 
Smith;  the domain expert reviewer, Elizabeth Hofer; the secondary reviewer, 
Tiago de Moraes Machado; the primary reviewer, Andrea Clemeno.

\subsection{Design Verification Plan}

The specifications and implementation of the software will be documented 
extensively with several reports including the SRS, VnV plan and VnV report. 
However, the design will not be documented as the modules of the code will be 
generated by the Drasil Framework. The generation of the design will be modular, 
correctness is assumed and verification is not required.

\subsection{Implementation Verification Plan}

The implementation of the software will be verified using several static methods involving manual or automated interactions. The software will be generated using the Drasil Framework to generate all of the software artifacts \citep{Drasil}. The generated python code 


The implementation of the software will be verified with several static methods 
involving manual or automated interactions. The software will be developed using 
the Drasil Framework to generate all of the software artifacts.\citet{Drasil}. 
The code 
developed uses python to achieve goals and fulfill requirements in the SRS. The 
design of the code will be evaluated by the project developer, domain expert, 
the secondary reviewer and the class instructor. The evaluation will involve 
code inspections where coding syntax, structure and standards are upheld. In 
addition, code walkthroughs will be performed where the evaluators will try to 
determine the output of the code using the code with little to no context. The 
code walkthrough will verify that the requirements and goals of the code are 
met. Moreover, automatic methods will verify the design by displaying success 
messages after certain checkpoints throughout the code. The mentioned tests will 
be explained in more detail in Section 5.


\subsection{Automated Testing and Verification Tools}

The \progname{} software will be tested and verified with several tools for 
unit and systems testing, static and dynamic analysis, linting and continuous 
integration. The static automatic testing will be completed in Spyder, a Python 
Integrated Development Environment, using several checkpoints with success and 
failure indicators. Additionally, the Spyder platform will analyze the code for 
potential errors in the process of linting. For testing performance optimization 
dynamically, a python profiler called cProfile will be used to profile speed 
(\citet{cProfile}).

Automated testing will be implemented for individual units as well as the 
integrated system to ensure that all the sections work separately and together 
seamlessly. In terms of unit  testing, each unit will be tested with the 
unittest python library within Spyder. Respectively, the systems test will be 
completed through black box testing with the Python Black Box tool called pbbt.

Lastly, the implementation of the design will be verified with continuous 
integration through Travis CI that is used in conjunction with Drasil. Drasil 
implements Travis CI to integrate code into a Github repository each day to 
complete automated tests to verify the code (\citet{Drasil}). 

\begin{table}[ht]
\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{Test} & \textbf{Verification Tool}\\
  \midrule 
  Static Analysis & Spyder\\
  Linting & Spyder\\
  Dynamic Analysis & cProfile \\
  System Test & Blackbox testing using pbbt\\
  Unit Test & Unittest within Spyder \\
  Continuous Integration & Travis CI\\
  \bottomrule
\end{tabular}\\
\end{center}
\caption{Automated Testing and Verification Tools}

\end{table}

\newpage

\subsection{Software Validation Plan}

The software validation plan will be implemented at the end of the development 
process to determine if the real world problem is characterized correctly. The 
validation of \progname{} will be completed by comparing the outputs of the 
software to several cases from scientific study called Viral Dynamics of Acute 
HIV-1 Infection seen in The Journal of Experimental Medicine 
(\citet{viraldynamics}).

\section{System Test Description}
	
\subsection{Tests for Functional Requirements}

This section will define the tests to ensure \progname{} meets the functional 
requirements seen in the SRS document for \progname{}. The subsections combine 
several requirements that are be separated based on common ideas.

\subsubsection{Testing inputs}

The user-defined inputs will undergo tests to ensure that numerical data was 
received and aligns with the input constraints. The tests will automatically 
display feedback if the conditions above are not met. The tests called 
id-added, input-received and input-verify are described in greater detail below: 

\paragraph{Input Testing}

\begin{center}
 \begin{tabular}{|| c||c c c c|| c ||} 
 \hline
 Test &$N_{o}$ (mol) & $N_{t}$ (mol) & $t_{t}$ (\si{\second}) & $t_{p}$ 
(\si{\second}) & Output\\ [0.5ex] 
 \hline
 Test 1-1 & 10000000 & 5000000 & 1 & 30 & - \\ 
 \hline
 Test-2-1 & 0 & 5000000 & 1 & 30 & Exception: InputError\\
 \hline
 Test-2-2 & -10000000 & 5000000 & 1 & 30 & Exception: InputError\\
 \hline
 Test-3-1 & 0 & 5000000 & 1 & 30 & Exception: InputError\\
 \hline
 Test-3-2 & -10000000 & 5000000 & 1 & 30 & Exception: InputError\\ 
 \hline
 Test 4-1 & 5000000 & 10000000 & 1 & 30 & Exception: InputError\\ 
 \hline
 Test-5-1 & 10000000 & 5000000 & 0 & 30 & Exception: InputError\\
 \hline
 Test-5-2 & 10000000 & 5000000 & -1 & 30 & Exception: InputError\\
 \hline
 Test-6-1 & 10000000 & 5000000 & 0 & 30 & Exception: InputError\\
 \hline
 Test-6-2 & 10000000 & 5000000 & -1 & 30 & Exception: InputError\\ 
 \hline
 Test 7-1 & 10000000 & 5000000 & 30 & 1 & Exception: InputError\\  [1ex] 
 \hline

\end{tabular}
\end{center}

\begin{enumerate}

\item{id-added\\}

Control: Automatic
					
Initial State: \progname{} running

Input:

\begin{itemize}
\item[Test input \refstepcounter{tinnum}\thetinnum\label{Tinput_1}:] $id$ = 
$123$% Test case where id is added
\item[Test input \refstepcounter{tinnum}\thetinnum\label{Tinput_2}:] $id$ = "   
"% Test case where no id detected
\end{itemize}


Output: 
\begin{itemize}
\item Test output 1: "id-added: success"
\item Test output 3: "id-added: failure. Try again."
\end{itemize}

Test Case Derivation:\\
The expected result for the given inputs will be either "id-added: success" or  
"id-added: failure. Try again." When the user inputs an id, the inputs will be 
received successfully. In comparison, any nonsensical data inputs like a null 
input will cause this test to output a failure.\\

How test will be performed: \\
This automatic static test will be completed in Spyder using if-then-else loop 
conditions to display the previously mentioned output. \\
\end{enumerate}

\paragraph{Input Testing}

\begin{enumerate}

\item{input-received\\}

Control: Automatic
					
Initial State: \progname{} running
					
Input:
\begin{itemize}
\item[Test input \refstepcounter{tinnum}\thetinnum\label{Tinput_3}:]$V_1$ = 
$5*10^8$, $V_2$ = $4*10^8$% Test case where V1>V2

\item[Test input \refstepcounter{tinnum}\thetinnum\label{Tinput_4}:] $V_1$ = 
$5*10^8$, $V_2$ = $6*10^8$% Test case where V1<V2

% Test case with two nonsensical data
\item[Test input \refstepcounter{tinnum}\thetinnum\label{Tinput_5}:] $V_1$ = 
abc123 , $V_2$ = $6*10^8$  
% Test case with two nonsensical data
\item[Test input \refstepcounter{tinnum}\thetinnum\label{Tinput_6}:] $V_1$ = 
abc123 , $V_2$ = 789xyz 
\end{itemize}

Output: 
\begin{itemize}
\item Test output 1: "input-received: success"
\item Test output 2: "input-received: success" 
\item Test output 3: "input-received: failure. Try again with numerical values."
\item Test output 4: "input-received: failure. Try again with numerical values."
\end{itemize}

Test Case Derivation:\\
The expected result for the given inputs will be either 
"input-received: success." or  "input-received: failure. Try again with 
numerical values." When the user-defined inputs are numbers, the inputs will be 
received successfully. In comparison, any nonsensical data inputs for one or 
more of the inputs will cause this test to output a failure.\\

How test will be performed: \\
This automatic static test will be completed in Spyder using if-then-else loop 
conditions to display the previously mentioned output. \\

					
\item{input-verify\\}

Control: Automatic
					
Initial State: \progname{} running and input-received is successful
					
Input:
\begin{itemize} % Test case where
\item[Test input \refstepcounter{tinnum}\thetinnum\label{Tinput_7}:] $V_1$ = 
$2*10^8$, $V_2$ = $5*10^8$ % V1<V2
\item[Test input \refstepcounter{tinnum}\thetinnum\label{Tinput_8}:]$V_1$ = 
$5*10^8$, $V_2$ = $5*10^8$% V1=V2
\item[Test input \refstepcounter{tinnum}\thetinnum\label{Tinput_9}:] $V_1$ = 
$5*10^8$, $V_2$ = $4*10^8$% V1>V2
\item[Test input \refstepcounter{tinnum}\thetinnum\label{Tinput_10}:]$V_1$ = 
$10*10^8$, $V_2$ = $6*10^8$% V1>>V2
\item[Test input \refstepcounter{tinnum}\thetinnum\label{Tinput_11}:]$V_1$ = 
$20*10^8$, $V_2$ = $5*10^8$% V1>>>V2

\end{itemize}

Output: 
\begin{itemize}
\item Test output 1: "input-verify: failure; $V_1$ should be greater than 
$V_2$."
\item Test output 2: "input-verify: failure; $V_1$ should be greater than 
$V_2$." 
\item Test output 3: "input-verify: success"
\item Test output 4: "input-verify: success"
\item Test output 5: "input-verify success"
\end{itemize}

Test Case Derivation:\\
The expected result for the given inputs will be either "input-verify: success" 
or  "input-verify: failure; $V_1$ should be greater than $V_2$." According to 
the input constraints specified in the SRS, the program can only determine the 
rate of clearance of the virus when the virus starts decreasing due the immune 
system affecting virus concentration. When the user-defined inputs include a 
greater $V_2$, the viral has yet to decrease and therefore, the input 
constraints are not met and the output is failure for test input 1.\\

How test will be performed: \\
This automatic static test will be completed in Spyder using if-then-else loop 
conditions to display the previously mentioned output. \\

\end{enumerate}
					
\subsubsection{Testing outputs}

The produced output will undergo tests to ensure that data is produced, aligns 
with the output constraints and is displayed to the user. The output-produced 
and output-valid tests will automatically display feedback if the conditions 
above are not met. The output-displayed test will be manual in the form of a 
checked box that will be checked by the user if the outputs are displayed. The 
tests called output-produced, output-verify and output-displayed are described 
in greater detail below: 
		
\paragraph{Output Testing}

\begin{enumerate}

\item{output-produced\\}

Control: Automatic
					
Initial State: \progname{} analysis completed 
					
Input:
\begin{itemize}
\item[Test input \refstepcounter{tinnum}\thetinnum\label{Tinput_12}:] 
$\dfrac{dV}{dt} = 1.3$ %  
\item[Test input \refstepcounter{tinnum}\thetinnum\label{Tinput_13}:]
$\dfrac{dV}{dt} = undefined$ % no value, null value
\item[Test input \refstepcounter{tinnum}\thetinnum\label{Tinput_14}:] 
$\dfrac{dV}{dt} = 0$ % no value, null value
\item[Test input \refstepcounter{tinnum}\thetinnum\label{Tinput_15}:] 
$\dfrac{dV}{dt} = -0.3$

\end{itemize}

Output: 
\begin{itemize}
\item Test output 1: "output-produced: success"
\item Test output 2: "output-produced: failure"
\item Test output 2: "output-produced: success" 
\item Test output 3: "output-produced: success"

\end{itemize}

Test Case Derivation:\\
The expected result for the given inputs will be either: "output-produced: 
success" or  "output-produced: failure". When the program produces an output, 
whether negative, zero or positive, a numerical value will have a successful 
result in this test. When an undefined result is produced, this test to output a 
failure.\\

How test will be performed: \\
This automatic static test will be completed in Spyder using if-then-else loop 
conditions to display the previously mentioned output. \\

					
\item{output-verify\\}

Control: Automatic
					
Initial State: \progname{} analysis completed; output-program is successful
					
Input:
\begin{itemize}
\item[Test input \refstepcounter{tinnum}\thetinnum\label{Tinput_16}:] 
$\dfrac{dV}{dt} = 1.3$ % rise 
\item[Test input \refstepcounter{tinnum}\thetinnum\label{Tinput_17}:] 
$\dfrac{dV}{dt} = 0$ % at peak
\item[Test input \refstepcounter{tinnum}\thetinnum\label{Tinput_18}:] 
$\dfrac{dV}{dt} = -0.3$
\item[Test input \refstepcounter{tinnum}\thetinnum\label{Tinput_19}:]
$\dfrac{dV}{dt} = -0.7$ 
\item[Test input \refstepcounter{tinnum}\thetinnum\label{Tinput_20}:]
$\dfrac{dV}{dt} = -0.9$ 
\end{itemize}


Output: 
\begin{itemize}
\item Test output 1: "output-verify: failure"
\item Test output 2: "output-verify: failure" 
\item Test output 3: "output-verify: success"
\item Test output 4: "output-verify: success"
\item Test output 5: "output-verify: success"
\end{itemize}

Test Case Derivation:\\
The expected result for the produced outputs will be either "output-verify: 
success" or  "output-verify: failure". According to the output constraints 
specified in the SRS, IM1 defines the rate of clearance of the virus is a 
negative value. When the outputproduced by the program ($\dfrac{dV}{dt}$) is 
greater than or equivalent to 0, the viral load has yet to decrease and 
therefore, the output constraints are not met and the test will fail.\\

How test will be performed: \\
This automatic static test will be completed in Spyder using if-then-else loop 
conditions to display the previously mentioned output. \\

\item{output-displayed\\}

Control: Manual
					
Initial State: \progname{} analysis completed; output-program , output-valid 
are successful
					
Input:
\begin{itemize} % Test case where
\item[Test input \refstepcounter{tinnum}\thetinnum\label{Tinput_21}:] 
$\dfrac{dV}{dt} = -0.3$, user indicates "displayed".
\item[Test input \refstepcounter{tinnum}\thetinnum\label{Tinput_22}:] 
$\dfrac{dV}{dt} = -0.9$, user indicates "displayed".
\item[Test input \refstepcounter{tinnum}\thetinnum\label{Tinput_23}:] 
$\dfrac{dV}{dt} = "     "$, user indicates "not displayed".
\end{itemize}

Output: 
\begin{itemize}
\item Test output 1: "output-displayed: success"
\item Test output 2: "output-displayed: success" 
\item Test output 3: "output-displayed: failure"
\end{itemize}

Test Case Derivation:\\
Possibilities of inputs and outputs of the test are identified above. In this 
test, the user will indicate if the output is displayed. The outputs of the test 
will be identified accordingly.\\

How test will be performed: \\
This manual static test will be completed in Spyder using user feedback to 
display the previously mentioned output. \\

\end{enumerate}

\subsection{Tests for Nonfunctional Requirements}
  
This section will define the tests to ensure \progname{} fulfill the 
nonfunctional requirements seen in the SRS document of \progname{}. The 
subsections are be separated for different requirements.

\subsubsection{Correctness}
		
\paragraph{Testing the Correctness of \progname{}:}
The correctness of the software can be defined using the constraints of 
\progname{}. Correctness of software is dependent on the satisfaction of the 
requirements in the SRS. The testing for the correctness of \progname{} will 
involve referencing input-verify and output-verify tests from the Tests for 
Functional Requirements.

\begin{enumerate}

\item{software-correctness\\}

Type: automatic
					
Initial State: \progname{} software run complete
					
Input:
\begin{itemize} % Test case where
\item Test input 1: input-verify = "success", output-verify = "success"
\item Test input 2: input-verify = "failure", output-verify = "success"
\item Test input 3: input-verify = "success", output-verify = "failure"
\end{itemize}

Output: 
\begin{itemize}
\item Test output 1: "software-correctness: $100\%$ correct"
\item Test output 2: "software-correctness: $50\%$ correct" 
\item Test output 3: "software-correctness: $50\%$ correct"
\end{itemize}
					
How test will be performed: \\
The correctness of the \progname{} will be tested in Spyder using if-then-else 
loop conditions to display the previously mentioned output. \\

\end{enumerate}

\subsubsection{Reliability}
\paragraph{Testing the Reliability of \progname{}:}
The reliability of the software tests if the product aligns with it's purpose. 
Reliability of \progname{} will be examined by comparing the output to several 
subject profiles from a scientific journal using relative error.
\begin{enumerate}

\item{software-reliability\\}

Type: automatic
					
Initial State: \progname{} software run complete
					
Condition:
\begin{itemize} % Test case where
\item $X$ = $(theoretical value - experimental value)$ / $experimental value$
\end{itemize}

Result: 
\begin{itemize}
\item The software is $X\%$ reliable.
\end{itemize}
				
					
How test will be performed: \\
The reliability of the \progname{} will be quantified in Spyder to display the 
previously mentioned output. \\

\end{enumerate}

\subsubsection{Usability}

\paragraph{Testing the Usability of \progname{}:}

The usability is highly important in the \progname{} software as many users 
will be interacting with the software. Testing the usability will determine if 
the users have a efficient interaction with the software. The system will be 
tested against a usability checklist and a usability survey to quantify the 
versatility of the user interface.


\begin{enumerate}

\item{software-usabilitychecklist\\}

Type: manual
					
Initial State: \progname{} software run complete
					
Input:
\begin{itemize} % Test case where
\item The design of the user interface and checklist.
\end{itemize}

Result: 
\begin{itemize}
\item The software aligns with $X\%$ of the checklist.
\end{itemize}
				
					
How test will be performed: \\
The usability of the \progname{} will be quantified manually by determining 
the percentage of items on the checklist that the software has.\\

\item{software-usabilitysurvey\\}

Type: manual
					
Initial State: \progname{} software run complete
					
Input:
\begin{itemize} % Test case where
\item The design of the user interface and survey questions.
\end{itemize}

Result: 
\begin{itemize}
\item A certain item should be discarded.
\item A certain item should be added.
\end{itemize}
				
					
How test will be performed: \\
The usability of the \progname{} will be quantified manually by determining 
possible changes from survey answers from the usability survey.\\

\end{enumerate}

\subsubsection{Performance}

\paragraph{Testing the Performance of \progname{}:}

The adequate time and memory performance is essential for the \progname{} 
software. Testing the performance will ensure the software is useful for 
intended users. 

\begin{enumerate}

\item{software-profiling\\}

Type: automatic
					
Initial State: \progname{} software run complete
					
Input:
\begin{itemize} % Test case where
\item The code generated by Drasil and cProfile library.
\end{itemize}

Result: 
\begin{itemize}
\item The wall time per function call
\item The cumulative time for a given function.
\end{itemize}

\end{enumerate}
				
					
How test will be performed: \\
The test for deterministic performance will use the cProfile library 
atuomatically in Spyder to identify lines of the code that can possibly be 
optimized. The system will be profiled using ProfileC to determine the wall time 
per function call and cumulative time spent on a given function.\\


\subsection{Traceability Between Test Cases and Requirements}

The purpose of the traceability matrices is to provide easy references on what
has to be additionally modified if a certain component is changed.  Every time a
component is changed, the items in the column of that component that are marked
with an ``X'' may have to be modified as well.  Table~\ref{Table:R_trace} shows 
which test cases are supporting which requirements.


\begin{landscape}
\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
	& R-Inputs & R-Constraints & R-AIDsdiagnosis & R-VerifyOutput 
& R-Output\\
\hline
\tinref{Tinput_1}        	&x & & & &  \\ 
\hline
\tinref{Tinput_2}			&x & & & &   \\ 
\hline
\tinref{Tinput_3}        	&x & & & &  \\ 
\hline
\tinref{Tinput_4}			&x & & & &   \\ 
\hline
\tinref{Tinput_5}        	&x & & & &  \\ 
\hline
\tinref{Tinput_6}			&x & & & &   \\ 
\hline
\tinref{Tinput_7}        	& &x & & &  \\ 
\hline
\tinref{Tinput_8}			& &x & & &   \\ 
\hline
\tinref{Tinput_9}        	& &x & & &  \\ 
\hline
\tinref{Tinput_10}			& &x & & &   \\ 
\hline
\tinref{Tinput_11}        	& &x & & &  \\ 
\hline
\tinref{Tinput_12}			& & &x & &   \\ 
\hline
\tinref{Tinput_13}        	& & &x & &  \\ 
\hline
\tinref{Tinput_14}			& & &x & &   \\ 
\hline
\tinref{Tinput_15}        	& & &x & &   \\  
\hline
\tinref{Tinput_16}			& & &x &x & \\ 
\hline
\tinref{Tinput_17}        	& & &x &x &   \\ 
\hline
\tinref{Tinput_18}			& & &x &x &   \\  
\hline
\tinref{Tinput_19}        	& & &x &x &   \\  
\hline
\tinref{Tinput_20}			& & &x &x &   \\  
\hline
\tinref{Tinput_21}        	& & & & &x  \\ 
\hline
\tinref{Tinput_22}			& & & & &x   \\ 
\hline
\tinref{Tinput_23}        	& & & & &x  \\ 
\hline
\end{tabular}
\caption{Traceability Matrix Showing the Connections Between Requirements and 
test cases}
\label{Table:R_trace}
\end{table}
\end{landscape}

\bibliographystyle{plainnat}

\bibliography{references.bib}
~\newpage

\newpage

\section{Appendix}

The appendix presents the usability checklist and survey questions mentioned in 
section.

\subsection{Usability Checklist}


\title{System Usability Checklist}


\begin{itemize}
  
\item Grammar, spelling, presentation
  \begin{itemize}
  \item No spelling mistakes 
  \item No grammar mistakes
  \item All hyperlinks work
  \item Symbolic names are used for quantities, rather than literal values
  \end{itemize}

\item Organization
  \begin{itemize}
  \item Page title for every page
  \item Step of process identified
  \item Name of step of process
  \end{itemize}
  
\item User Interaction
  \begin{itemize}
  \item Field names 
  \item Obvious field location 
  \item Important information highlighted
  \item Help or Suggestion for each page
  \end{itemize}
\end{itemize}


\subsection{Usability Survey Questions}

This is a section states the usability questions to ask for the manual test for 
the usability requirement.

\begin{enumerate}
  \item What do you like most about the interface?
  \item What would you like to change about the interface?
  \item Did you face any challenge while using the site? 
  \item Were the buttons and fields easy to find and understand?
  \item On a scale of 1-10, how easy was it to navigate through the interface? 
  \item Did you feel that the software took too long to load the website?
  \item Did you feel that the software took too long to fetch your details on 
our website?
  \item Do you have any suggestions or comments?
\end{enumerate}



\end{document}









